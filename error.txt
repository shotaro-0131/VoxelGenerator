WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 00:20:48,292][0m A new study created in memory with name: no-name-a3f0759f-6dca-4280-9066-1cd6a6acdfcf[0m
GPU available: True, used: True
TPU available: None, using: 0 TPU cores

  | Name  | Type    | Params
----------------------------------
0 | model | UNet    | 18.6 M
1 | loss  | VAELoss | 0     
----------------------------------
18.6 M    Trainable params
0         Non-trainable params
18.6 M    Total params
74.324    Total estimated model params size (MB)
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 01:49:28,720][0m A new study created in memory with name: no-name-be286561-8abb-4899-9df3-d961c9e46053[0m
GPU available: True, used: True
TPU available: None, using: 0 TPU cores

  | Name  | Type    | Params
----------------------------------
0 | model | UNet    | 123 M 
1 | loss  | VAELoss | 0     
----------------------------------
123 M     Trainable params
0         Non-trainable params
123 M     Total params
494.661   Total estimated model params size (MB)
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 02:02:07,842][0m A new study created in memory with name: no-name-0477c06b-03dc-4dd5-8491-a7ebd7333138[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 02:02:18,020][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 128, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 135, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 128, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 11:44:07,490][0m A new study created in memory with name: no-name-9e720513-91c3-4d58-8c83-567376caa4ea[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 11:44:15,846][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 128, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 135, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 128, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 12:04:43,537][0m A new study created in memory with name: no-name-983cf825-0160-4588-9081-015d711e2ac5[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 12:04:51,097][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 136, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 12:20:58,712][0m A new study created in memory with name: no-name-be17094b-5bc3-4426-884c-2eb107de3583[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 12:21:06,663][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 136, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 12:23:25,564][0m A new study created in memory with name: no-name-69e826b3-318e-49d2-b52b-edb2ceaf97d7[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 12:23:32,193][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 136, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 12:40:19,204][0m A new study created in memory with name: no-name-fabb1ab9-9e2b-469a-825c-f8ba076dd293[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 12:40:27,537][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 136, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 129, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 12:42:45,624][0m A new study created in memory with name: no-name-cf906359-cd77-4808-9c97-901d047b26d4[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 12:42:53,287][0m Trial 0 failed because of the following error: NameError("name 'unwrap_lightning_module_sharded' is not defined",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 442, in fit
    self.model_connector.copy_trainer_model_properties(model)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/model_connector.py", line 28, in copy_trainer_model_properties
    ref_model = self.trainer.lightning_module or model
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/properties.py", line 358, in lightning_module
    return self.accelerator.lightning_module
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 109, in lightning_module
    return self.training_type_plugin.lightning_module
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/sharded.py", line 59, in lightning_module
    return unwrap_lightning_module_sharded(self._model)
NameError: name 'unwrap_lightning_module_sharded' is not defined[0m
Traceback (most recent call last):
  File "model_fine.py", line 137, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 442, in fit
    self.model_connector.copy_trainer_model_properties(model)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/model_connector.py", line 28, in copy_trainer_model_properties
    ref_model = self.trainer.lightning_module or model
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/properties.py", line 358, in lightning_module
    return self.accelerator.lightning_module
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 109, in lightning_module
    return self.training_type_plugin.lightning_module
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/sharded.py", line 59, in lightning_module
    return unwrap_lightning_module_sharded(self._model)
NameError: name 'unwrap_lightning_module_sharded' is not defined

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 13:06:29,720][0m A new study created in memory with name: no-name-a425e5c8-e5a3-4201-94c9-fb2c1c25048e[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py:398: FutureWarning: `n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.
  FutureWarning,
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 13:06:40,464][0m Trial 1 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
[33m[W 2021-07-17 13:06:40,465][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 137, in main
    study.optimize(objective, n_trials=100, n_jobs=cfg.training.gpu_num)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 105, in _optimize
    f.result()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
  File "model_fine.py", line 137
    with parallel_backend("multiprocessing", n_jobs=cfg.training.gpu_num)
                                                                        ^
SyntaxError: invalid syntax
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Traceback (most recent call last):
  File "model_fine.py", line 14, in <module>
    from joblib import parallel_backend
ModuleNotFoundError: No module named 'joblib'
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 13:11:05,404][0m A new study created in memory with name: no-name-b16266fc-eca0-4411-87e6-673e8f93846d[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py:398: FutureWarning: `n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.
  FutureWarning,
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 13:11:12,981][0m Trial 1 failed because of the following error: TypeError("__init__() missing 2 required positional arguments: 'op' and 'message'",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 192, in makedirs
    os.makedirs(path)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/os.py", line 220, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/home/4/16B14840/voxelcnn/VoxelLigandGenerator/output/lightning_logs/version_43'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 201, in log_metrics
    self.experiment.add_scalar(k, v, step)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/base.py", line 41, in experiment
    return get_experiment() or DummyExperiment()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py", line 41, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/base.py", line 39, in get_experiment
    return fn(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 142, in experiment
    self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py", line 220, in __init__
    self._get_file_writer()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py", line 251, in _get_file_writer
    self.flush_secs, self.filename_suffix)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py", line 61, in __init__
    log_dir, max_queue, flush_secs, filename_suffix)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/summary/writer/event_file_writer.py", line 73, in __init__
    tf.io.gfile.makedirs(logdir)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 665, in makedirs
    return get_filesystem(path).makedirs(path)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 195, in makedirs
    None, None, "Directory already exists"
tensorboard.compat.tensorflow_stub.errors.AlreadyExistsError: Directory already exists

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 125, in objective
    trainer.logger.log_hyperparams(hyperparameters)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py", line 41, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 180, in log_hyperparams
    self.log_metrics(metrics, 0)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py", line 41, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 205, in log_metrics
    type(ex)(ex.message + m)
TypeError: __init__() missing 2 required positional arguments: 'op' and 'message'[0m
[33m[W 2021-07-17 13:11:13,295][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 192, in makedirs
    os.makedirs(path)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/os.py", line 220, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/home/4/16B14840/voxelcnn/VoxelLigandGenerator/output/lightning_logs/version_43'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 201, in log_metrics
    self.experiment.add_scalar(k, v, step)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/base.py", line 41, in experiment
    return get_experiment() or DummyExperiment()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py", line 41, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/base.py", line 39, in get_experiment
    return fn(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 142, in experiment
    self._experiment = SummaryWriter(log_dir=self.log_dir, **self._kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py", line 220, in __init__
    self._get_file_writer()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py", line 251, in _get_file_writer
    self.flush_secs, self.filename_suffix)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py", line 61, in __init__
    log_dir, max_queue, flush_secs, filename_suffix)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/summary/writer/event_file_writer.py", line 73, in __init__
    tf.io.gfile.makedirs(logdir)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 665, in makedirs
    return get_filesystem(path).makedirs(path)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 195, in makedirs
    None, None, "Directory already exists"
tensorboard.compat.tensorflow_stub.errors.AlreadyExistsError: Directory already exists

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "model_fine.py", line 138, in main
    study.optimize(objective, n_trials=100, n_jobs=cfg.training.gpu_num)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 105, in _optimize
    f.result()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 125, in objective
    trainer.logger.log_hyperparams(hyperparameters)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py", line 41, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 180, in log_hyperparams
    self.log_metrics(metrics, 0)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py", line 41, in wrapped_fn
    return fn(*args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py", line 205, in log_metrics
    type(ex)(ex.message + m)
TypeError: __init__() missing 2 required positional arguments: 'op' and 'message'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 13:16:41,062][0m A new study created in memory with name: no-name-3f2e8798-0eb9-4f74-bb78-332baa2bb031[0m
[33m[W 2021-07-17 13:16:41,883][0m Trial 0 failed because of the following error: MisconfigurationException('You requested GPUs: [0, 1]\n But your machine only has: []',)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 122, in objective
    callbacks=[PyTorchLightningPruningCallback(trial, monitor="val_loss")])
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py", line 39, in insert_env_defaults
    return fn(self, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 320, in __init__
    replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 117, in __init__
    self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/device_parser.py", line 81, in parse_gpu_ids
    gpus = _sanitize_gpu_ids(gpus)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/device_parser.py", line 143, in _sanitize_gpu_ids
    f"You requested GPUs: {gpus}\n But your machine only has: {all_available_gpus}"
pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0, 1]
 But your machine only has: [][0m
Traceback (most recent call last):
  File "model_fine.py", line 138, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 122, in objective
    callbacks=[PyTorchLightningPruningCallback(trial, monitor="val_loss")])
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py", line 39, in insert_env_defaults
    return fn(self, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 320, in __init__
    replace_sampler_ddp, deterministic, precision, amp_backend, amp_level, plugins
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 117, in __init__
    self.parallel_device_ids = device_parser.parse_gpu_ids(self.gpus)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/device_parser.py", line 81, in parse_gpu_ids
    gpus = _sanitize_gpu_ids(gpus)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/device_parser.py", line 143, in _sanitize_gpu_ids
    f"You requested GPUs: {gpus}\n But your machine only has: {all_available_gpus}"
pytorch_lightning.utilities.exceptions.MisconfigurationException: You requested GPUs: [0, 1]
 But your machine only has: []

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 13:17:56,042][0m A new study created in memory with name: no-name-11bb4a4c-1a18-473a-ac21-70fd5b2ab33c[0m
/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(accelerator="dp"|"ddp"|"ddp2")`. Setting `accelerator="ddp_spawn"` for you.
  warnings.warn(*args, **kwargs)
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
[33m[W 2021-07-17 13:18:04,306][0m Trial 0 failed because of the following error: TypeError("can't pickle dict_items objects",)
Traceback (most recent call last):
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects[0m
Traceback (most recent call last):
  File "model_fine.py", line 138, in main
    study.optimize(objective, n_trials=100)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/study.py", line 410, in optimize
    show_progress_bar=show_progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 75, in _optimize
    progress_bar=progress_bar,
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 162, in _optimize_sequential
    trial = _run_trial(study, func, catch)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 267, in _run_trial
    raise func_err
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/optuna/_optimize.py", line 216, in _run_trial
    value_or_values = func(trial)
  File "model_fine.py", line 130, in objective
    trainer.fit(model, dataloader, val_dataloader)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 489, in safe_patch_function
    patch_function(call_original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 240, in patch_with_managed_run
    result = patch_function(original, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 303, in fit
    return _run_and_log_function(self, original, args, kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/pytorch/_pytorch_autolog.py", line 295, in _run_and_log_function
    result = original(self, *args, **kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/mlflow/utils/autologging_utils/safety.py", line 445, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 498, in fit
    self.dispatch()
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py", line 545, in dispatch
    self.accelerator.start_training(self)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py", line 73, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py", line 106, in start_training
    mp.spawn(self.new_process, **self.mp_spawn_kwargs)
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/4/16B14840/.local/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 179, in start_processes
    process.start()
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/process.py", line 105, in start
    self._popen = self._Popen(self)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/context.py", line 284, in _Popen
    return Popen(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 32, in __init__
    super().__init__(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/popen_spawn_posix.py", line 47, in _launch
    reduction.dump(process_obj, fp)
  File "/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/lib/python3.6/multiprocessing/reduction.py", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle dict_items objects

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[W CudaIPCTypes.cpp:21] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.
You should consider upgrading via the '/home/4/16B14840/.pyenv/versions/anaconda3-4.0.0/envs/py36/bin/python3 -m pip install --upgrade pip' command.
Global seed set to 0
[32m[I 2021-07-17 13:19:27,807][0m A new study created in memory with name: no-name-0e444dcf-bcc8-4c19-bd0e-195afcea5649[0m
GPU available: True, used: True
TPU available: None, using: 0 TPU cores
Global seed set to 0
[32m[I 2021-07-17 13:19:38,789][0m A new study created in memory with name: no-name-a2c87dd7-9221-4391-b66f-19e0f76cf92f[0m
Global seed set to 0
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
2021/07/17 13:19:42 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: 'NoneType' object is not callable
Global seed set to 0
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
